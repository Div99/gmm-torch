{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cddf967-1d0d-497c-a3e9-304c2d366f24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%autoreload 9\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gmm\n",
    "from example import plot\n",
    "from gumbel_regression import *\n",
    "\n",
    "def create_data():\n",
    "    np.random.seed(0)\n",
    "    n_per_cluster = 40\n",
    "    means = np.array([[0,0], [-5,5], [5,5]])\n",
    "    X = np.vstack([np.random.randn(n_per_cluster, 2) + mu for mu in means])\n",
    "    true_y = np.array([0] * n_per_cluster + [1] * n_per_cluster + [2] * n_per_cluster)\n",
    "    return torch.Tensor(X), true_y\n",
    "\n",
    "def create_data2():\n",
    "    np.random.seed(0)\n",
    "    n_per_cluster = 50\n",
    "    means = np.array([[0,0], [-5, -5]])\n",
    "    X = np.vstack([np.random.randn(n_per_cluster, 2) + mu for mu in means])\n",
    "    true_y = np.array([0] * n_per_cluster + [1] * n_per_cluster)\n",
    "    return torch.Tensor(X), true_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f75bf9f-29ec-4d9d-a76a-022aca1b6caf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-1.1261,  0.0650],\n",
      "         [-0.9060,  1.4437]]])\n",
      "tensor([-1.1261,  0.0650])\n",
      "Parameter containing:\n",
      "tensor([[[-1.1261,  0.0650],\n",
      "         [-0.9060,  1.4437]]])\n",
      "tensor([-0.9060,  1.4437])\n",
      "Loss: tensor(-2.6511, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-2.6165, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-1.7313, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-1.1519, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-0.7860, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-0.5231, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-0.2654, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-0.1349, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-0.0757, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-0.0309, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.0032, 0.3322],\n",
      "         [0.0940, 0.3788]]])\n",
      "tensor([0.0032, 0.3322])\n",
      "Parameter containing:\n",
      "tensor([[[0.0032, 0.3322],\n",
      "         [0.0940, 0.3788]]])\n",
      "tensor([0.0940, 0.3788])\n",
      "Loss: tensor(-0.0137, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(-0.0022, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0079, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0096, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0097, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0100, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3906],\n",
      "         [0.1405, 0.3905]]])\n",
      "tensor([0.1404, 0.3906])\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3906],\n",
      "         [0.1405, 0.3905]]])\n",
      "tensor([0.1405, 0.3905])\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3905]]])\n",
      "tensor([0.1404, 0.3905])\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3905]]])\n",
      "tensor([0.1405, 0.3905])\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3905]]])\n",
      "tensor([0.1404, 0.3905])\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3905]]])\n",
      "tensor([0.1405, 0.3905])\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1404, 0.3905])\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1405, 0.3904])\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1404, 0.3905])\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1405, 0.3904])\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1404, 0.3905])\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1405, 0.3904])\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1404, 0.3905])\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1405, 0.3904])\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1404, 0.3905])\n",
      "Parameter containing:\n",
      "tensor([[[0.1404, 0.3905],\n",
      "         [0.1405, 0.3904]]])\n",
      "tensor([0.1405, 0.3904])\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n",
      "Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gumbel_stable_loss(z, clip=None):\n",
    "    if clip is not None:\n",
    "        z = torch.clamp(z, max=clip)\n",
    "\n",
    "    max_z = torch.max(z)\n",
    "    max_z = torch.where(max_z < -1.0, torch.tensor(-1.0, dtype=torch.double, device=max_z.device), max_z)\n",
    "    max_z = max_z.detach()  # Detach the gradients\n",
    "    \n",
    "    # scale by e^max_z\n",
    "    loss = torch.exp(z - max_z) - z * torch.exp(-max_z) - torch.exp(-max_z)\n",
    "    return loss\n",
    "\n",
    "def loss_fn(V, logP):\n",
    "    # loss = gumbel_stable_loss(logP - V, clip=10) # + logP\n",
    "\n",
    "    loss = torch.mean(torch.exp(logP-V) - V - 1)\n",
    "    z = logP - V\n",
    "\n",
    "    max_z = torch.max(z)\n",
    "    max_z = torch.where(max_z < -1.0, torch.tensor(-1.0, dtype=torch.double, device=max_z.device), max_z)\n",
    "    max_z = max_z.detach()  # Detach the gradients\n",
    "    loss = loss * torch.exp(-max_z)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    data, true_y = create_data2()\n",
    "    n = len(data)\n",
    "    d = data.shape[1]\n",
    "    k = 2\n",
    "    g_fitter = gmm.GaussianMixture(n_components=k, n_features=d)\n",
    "    max_iter = 100\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "\n",
    "    V = torch.zeros(n, requires_grad=True) \n",
    "    V_mean = torch.mean(V)\n",
    "    V_lr = 0.01\n",
    "    V_optim = torch.optim.Adam([V], lr=V_lr)\n",
    "    # loss_fn = gumbel_stable_loss\n",
    "\n",
    "    logP = g_fitter._estimate_log_prob(data)\n",
    "    loss = loss_fn(V, logP)  # TODO: Do we need to compute mean here?\n",
    "    writer.add_scalar('loss', loss, 0)\n",
    "    \n",
    "    # Randomly initialize means\n",
    "    g_fitter.mu = torch.nn.Parameter(torch.randn(1, k, d), requires_grad=False)\n",
    "\n",
    "    for iter_ in range(max_iter):\n",
    "        pred_y = g_fitter.predict(data)\n",
    "        if iter_ % 10 == 0:\n",
    "            plot(data, true_y, pred_y, iter_, g_fitter.mu)\n",
    "\n",
    "        # Do V step\n",
    "        logP = g_fitter._estimate_log_prob(data)\n",
    "\n",
    "        V_optim.zero_grad()\n",
    "        # loss = torch.mean(loss_fn(data-V))\n",
    "        loss = loss_fn(V, logP)\n",
    "        loss = loss.mean()\n",
    "        writer.add_scalar('loss', loss, iter_+1)\n",
    "        loss.backward(retain_graph=True)\n",
    "        V_optim.step()\n",
    "        print(\"Loss:\", loss, \"\\n\")\n",
    "        \n",
    "        # Do theta step\n",
    "        weighted_log_prob = logP + np.log(1/k) * torch.ones_like(logP)\n",
    "        log_prob_norm = V.unsqueeze(1).unsqueeze(2)\n",
    "        log_resp = weighted_log_prob - log_prob_norm\n",
    "        pi, mu, var = g_fitter._m_step(data, log_resp)\n",
    "        g_fitter.update_pi(pi)\n",
    "        g_fitter.update_mu(mu)\n",
    "        g_fitter.update_var(var)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4076b6-50b0-4335-8fe1-c20a4e4e8e0c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048adf1-8b5f-45f8-a00f-d3101731625b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
